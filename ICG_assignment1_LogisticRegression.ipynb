{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICG_assignment1_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWT8Etz5l5L59JsATkTefH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geeksid1800/ICG_ass1_logisticRegression/blob/main/ICG_assignment1_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UegQtkGpBHMy"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "#print(train_data[\"Age\"].isnull().sum())\n",
        "\n",
        "#mod_train_data = train_data.dropna(subset=['Age'])\n",
        "mod_train_data = train_data\n",
        "tr_med = mod_train_data['Age'].median(skipna=True);\n",
        "#print(\"median \",tr_med)\n",
        "mod_train_data.fillna(tr_med,inplace=True)\n",
        "mod_train_data['Sex'].replace(['male','female'],[1,2],inplace=True)\n",
        "\n",
        "mod_test_data = test_data\n",
        "ts_med = mod_test_data['Age'].median(skipna=True);\n",
        "#print(\"median \",ts_med)\n",
        "mod_test_data.fillna(ts_med,inplace=True)\n",
        "mod_test_data['Sex'].replace(['male','female'],[1,2],inplace=True)\n",
        "\n",
        "mod_train_data.to_csv('mod_train_data.csv',index=False)\n",
        "mod_test_data.to_csv('mod_test_data.csv',index=False)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol6dKjIxnYmf"
      },
      "source": [
        "#plotting helper functions\n",
        "\n",
        "def plot_points(X, y):\n",
        "    survived = X[np.argwhere(y==1)]\n",
        "    dead = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][3] for s in dead], [s[0][6] for s in dead], s = 25, color = 'blue', edgecolor = 'g')\n",
        "    plt.scatter([s[0][3] for s in survived], [s[0][6] for s in survived], s = 25, color = 'red', edgecolor = 'k')\n",
        "\n",
        "def display(m, b, color='g--'):\n",
        "    plt.xlim(-0.05,1.05)\n",
        "    plt.ylim(-0.05,1.05)\n",
        "    x = np.arange(-10, 10, 0.1)\n",
        "    plt.plot(x, m*x+b, color)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlFWXWxZnYmm"
      },
      "source": [
        "tr_data = pd.read_csv(r'mod_train_data.csv')\n",
        "X_train = np.array(tr_data.iloc[:, np.r_[0,2:8]])\n",
        "y_train = np.array(tr_data.iloc[:,1])\n",
        "\n",
        "ts_data = pd.read_csv(r'mod_test_data.csv')\n",
        "X_test = np.array(ts_data.iloc[:, np.r_[0,2:8]])\n",
        "y_test = np.array(ts_data.iloc[:,1])\n",
        "\n",
        "#plot_points(X,y)\n",
        "#plt.show()\n",
        "#print(X[:10])\n",
        "#print(y[:10])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCzDMvOez67v"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def gradient_descent(X, y, params, learning_rate, iterations):\n",
        "    m = len(y)\n",
        "    s = int(iterations/10)\n",
        "    cost_history = np.zeros((s,1))\n",
        "\n",
        "    for i in range(iterations):\n",
        "        params = params - (learning_rate/m) * (X.T @ (sigmoid(X @ params) - y))\n",
        "        if not i%10:\n",
        "          plt_itr = int(i/10)\n",
        "          cost_history[plt_itr] = compute_cost(X, y, params)\n",
        "\n",
        "    return (cost_history, params)\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X @ theta)\n",
        "    epsilon = 1e-5\n",
        "    cost = (1/m)*(((-y).T @ np.log(h + epsilon))-((1-y).T @ np.log(1-h + epsilon)))\n",
        "    return cost\n",
        "\n",
        "def predict(X, params):\n",
        "    return np.round(sigmoid(X @ params))\n",
        "\n",
        "m = len(y_train)    #no. of training examples\n",
        "X = np.hstack((np.ones((m,1)),X_train))  #add a column for the bias vector input, ie feature x0\n",
        "list_imp = [1,4,7]\n",
        "\n",
        "X_addn = np.array([])\n",
        "for i in list_imp:\n",
        "  for j in list_imp:\n",
        "    temp = np.multiply(X[:,i], X[:,j])\n",
        "    temp = temp[:,np.newaxis]\n",
        "    if X_addn.size==0:\n",
        "      X_addn = temp;\n",
        "    else:\n",
        "      X_addn = np.hstack((X_addn,temp))\n",
        "X = np.hstack((X,X_addn))\n",
        "y = y_train[:,np.newaxis]\n",
        "n = np.size(X,1)  #no. of given features+1. ie no. of columns+1\n",
        "params = np.zeros((n,1))\n",
        "\n",
        "sns.set_style('white')\n",
        "sns.scatterplot(X[:,1],X[:,7],hue=y.reshape(-1));  #scatter plot of (ID, fare) for survival/death\n",
        "\n",
        "\n",
        "iterations = 300\n",
        "learning_rate = 0.1\n",
        "\n",
        "initial_cost = compute_cost(X, y, params)\n",
        "\n",
        "print(\"Initial Cost is: {} \\n\".format(initial_cost))\n",
        "\n",
        "(cost_history, params_optimal) = gradient_descent(X, y, params, learning_rate, iterations)\n",
        "\n",
        "print(\"Optimal Parameters are: \\n\", params_optimal, \"\\n\")\n",
        "#print(cost_history[-10:])\n",
        "plt.figure()\n",
        "sns.set_style('white')\n",
        "plt.plot(range(len(cost_history)), cost_history, 'r')\n",
        "plt.title(\"Convergence Graph of Cost Function\")\n",
        "plt.xlabel(\"Number of Iterations/100\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "m1 = len(y_test)\n",
        "X = np.hstack((np.ones((m1,1)),X_test))\n",
        "\n",
        "X_addn = np.array([])\n",
        "for i in list_imp:\n",
        "  for j in list_imp:\n",
        "    temp = np.multiply(X[:,i], X[:,j])\n",
        "    temp = temp[:,np.newaxis]\n",
        "    if X_addn.size==0:\n",
        "      X_addn = temp;\n",
        "    else:\n",
        "      X_addn = np.hstack((X_addn,temp))\n",
        "X = np.hstack((X,X_addn))\n",
        "y = y_test[:,np.newaxis]\n",
        "n1 = np.size(X,1)\n",
        "\n",
        "y_pred = predict(X, params_optimal)\n",
        "score = float(sum(y_pred == y))/ float(len(y))\n",
        "\n",
        "print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
        "print(\"The score is \",score)\n",
        "\n",
        "slope = -(params_optimal[1] / params_optimal[2])\n",
        "intercept = -(params_optimal[0] / params_optimal[2])\n",
        "\n",
        "sns.set_style('white')\n",
        "sns.scatterplot(X[:,1],X[:,2],hue=y.reshape(-1));\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}